# DynQCD

DynQCD is a software suite for numerical simulations of lattice quantum
chromodynamics. It is mainly used in physics projects of the
[Budapest-Marseille-Wuppertal](https://www.bmw.uni-wuppertal.de) lattice
collaboration. Using this code, the computation of the muon magnetic moment has
been performed recently, with the result published in
[Nature](https://www.nature.com/articles/s41586-021-03418-1). The code is
written in GNU flavor of C99 and parallelized with OpenMP and MPI.

The benchmark below performs conjugate gradient iterations.

## Source

Archive Name: `dynqcd-bench.tar.gz`

The source code is available under `src/dynqcd/`. The code is **not** Open Source Software,
cannot be distributed further, and is only available here for the purpose of running the
procurement benchmark for JUPITER.

## Building

DynQCD can be simply compiled with

```bash
cd src/dynqcd
make -j HAVE_FFTW=0 HAVE_LAPACK=0
```

This should produce an executable, `src/dynqcd/dynqcd`. FFTW and LAPACK are not
needed for the test and have to be disabled for simplicity of the compilation.

The JUBE step `compile`, triggered when calling JUBE (see below), automatically compiles DynQCD.

## Execution

To run the code, input files have to created which are dependent on the numbers 
of nodes, tasks, and threads. The input file is generated by calling the 
`params.sh` script. The first argument is the number of nodes, the second is 
the number of targeted cores per node (this might be less than what is actually 
present in the hardware). Using these as input, the script provides valid 
configurations differing in the number of MPI tasks per node. From the 
configurations, the user has to choose, and upon choice, the input files are 
created.
Finally the code can be executed via the usual MPI launcher. Specifically

```bash
cd src
bash ./params.sh <nodes> <ncpuspernode>  # select configuration
export OMP_NUM_THREADS=<ncpuspertask>
[mpiexec] ./dynqcd/dynqcd parameters &> stdout
```

After initialization, the application runs several instances of conjugate
gradient inversions. Timings and per-thread performance metrics are written to
the stdout.

### Multithreading

DynQCD employs OpenMP for parallization and MPI for distribution.  The desired
number of threads per task can be set by the ``OMP_NUM_THREADS`` environment
variable. The benchmarker has to choose the number of tasks per node and the
number of cpus per task in accordance with the `params.sh` script.

### JUBE

Using JUBE, the building and execution steps can be executed via the command

```bash
jube run benchmark/jube/default.xml
```

Prerequisite for the execution is the succesful generation of a parameters file
as above. The desired number of nodes, number of tasks per node, number of
threads per task have to be specified in `default.xml`. These have to be
consistent with the application of  the `params.sh` script.

## Verification

For verification, a verify script should be called with the stdout of the execute step as argument

```bash
cd src
./verify.sh stdout
```

The script returns `0` on success, otherwise an error message is dropped. It
checks if the required amount of inversions have been done and if they reached
the required precision.

With JUBE, the verification is automatically executed as part of the
post-processing and displayed in the result table.

## Results

The proper runtime of the application can be grepped from the stdout as

```bash
grep 'stn0pf: eigcg sec=' stdout
```

which gives the total time spent in the conjugate gradient iterations.

### JUBE

Using JUBE the result can be accessed as

```bash
jube result -a benchmark/jube/run
```

| Nodes | Tasks/Node | Threads/Task | Runtime / s | Verified |
|-------|------------|--------------|-------------|----------|
|     8 |         32 |            4 |       261.0 |     True |

## Baseline

The baseline configuration must be chosen such that the runtime of the
application as defined above, is less or equal to 261 seconds. The value was
achieved on JURECA DC with 8 nodes and 32 MPI tasks per node each with 4 OpenMP
threads.
